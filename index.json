[{"content":"Introduction In this post we will setup loadbalancers in High Availability mode. We will be setting up the load balancers in active-passive fail-over configuration.\nI will be using haproxy as the load balancer and apache as the web server, but you can try with nginx as well.\nIn order to setup active-passive configuration we need some process which periodically checks the active load balancer and as soon as it goes down, it should promote the passive one to be active. For this we will be using keepalived.\nPrerequisites In order to proceed, we need 4 machines. All the 4 machines should be in the same network and should have internet access to install the required packages. Out of these 4 machines, 2 are for load balancers(active and passive) and 2 will be our web servers.\nFor this I will be using virtual machines connected via a virtual box NAT network. If you already have machines in hand, please skip to Web server setup section.\nVirtualbox VM setup \u0026lt;\u0026mdash; click to expand Virtualbox VM setup Step 1 Go to Network Manager Step 2 Create a new NAT network Step 3 Create a new VM. I will be using ubuntu server. I just grabbed the VM from https://www.linuxvmimages.com/images/ubuntuserver-2204/\nStep 4 Clone the VM to have a total of 4 VMs.\nStep 5 Set NAT network on the 4 VMs Step 6 Boot into the nodes and edit hostnames and note the IP address.\nI have named the load balancer nodes are load-balancer-1 \u0026amp; load-balancer-2 and the web server nodes as web-server-1 \u0026amp; web-server-2.\nNote: If cloning the VMs you might see the same IP addressess being assigned to all the clones. This is because of /etc/machine-id file present on the nodes. To solve this problem you can delete the file rm -f /etc/machine-id and recreate it dbus-uuidgen --ensure=/etc/machine-id.\nIf /var/lib/dbus/machine-id also exists as regular file, remove it as well rm /var/lib/dbus/machine-id and recreate it dbus-uuidgen --ensure. This last command implicitly uses /var/lib/dbus/machine-id as the file name and will copy the machine ID from the already-newly-generated /etc/machine-id [[4]].\nFinally you will have 4 different machines in the same network with different IPs Web server setup On both web servers install apache2.\nsudo apt update \u0026amp;\u0026amp; sudo apt install -y apache2 Next we configure the web server\necho \u0026#34;Hello from web server 1\u0026#34; \u0026gt; /var/www/html/index.html Start and enable the web server\nsudo systemctl enable --now apache2 Load balancer setup Install and setup haproxy On both load balancer nodes install haproxy sudo apt update \u0026amp;\u0026amp; sudo apt install -y haproxy Next we need to add frontend and backend configs to haproxy.\nEdit the config file at /etc/haproxy/haproxy.cfg appending in the following:- frontend mysite bind *:80 option tcplog mode tcp default_backend web-servers backend web-servers mode tcp balance roundrobin option tcp-check server web1 10.0.2.6:80 check fall 3 rise 2 server web2 10.0.2.7:80 check fall 3 rise 2 The HAProxy config on both LB nodes should look like below after you are done. The fall parameter sets how many failed checks are allowed. The rise parameter sets how many passing checks there must be before returning a previously failed server to the rotation. Refer [5] for more health check options.\nRestart and enable the haproxy service sudo systemctl restart haproxy sudo systemctl enable haproxy Right now if we were to visit any of the addresses below on any of these 4 nodes(or any other node in the same network), we should get our HTML page back.\ncurl http://10.0.2.4 # load-balancer-1 curl http://10.0.2.5 # load-balancer-2 curl http://10.0.2.6 # web-server-1 curl http://10.0.2.7 # web-server-2 Next we need to enable failover, i.e., in case one of the load balancer goes down, the web page should still be reachable.\nFor this we will need some process to monitor haproxy on the LB nodes and somehow route the requests via loadbalancer which is alive. This can be done using Virtual Router Redundancy Protocol(VRRP).\nInstall and setup keepalived We will use Keepalived, which is a software implementation of VRRP on Linux.\nBefore setting up keepalived let\u0026rsquo;s understand the request flow. VRRP uses a virtual IP address (VIP) [6]. A set of routers participate in an election to determine the host that will control that VIP. Only one router (the master) controls the VIP at a time. If the master fails, VRRP provides mechanisms for detecting that failure and quickly failing over to a standby router. The same algorithm can be used for servers as well.\nIn the above topology, lb1 is the master and is responsible for the 10.0.2.100 IP address. If lb1 fails, then lb2 takes over this IP. This is why it is also called floating IP.\nOn both LB nodes install keepalived\nsudo apt update \u0026amp;\u0026amp; sudo apt install -y keepalived Next we need to configure keepalived. As mentioned before, keepalived needs to monitor haproxy service running on the node, and as soon as it find haproxy is down on the node, it needs to switch the active and the passive LB nodes by moving the virtual IP from MASTER to BACKUP node.\nIn order to check whether LB went down we can check for the status of haproxy. For this we can use kill command with -0 flag. [1]\nman kill ... DESCRIPTION ... If signal is 0, then no actual signal is sent, but error checking is still performed. We can use this to check whether our load balancer is down.\nNext we configure keepalived by creating /etc/keepalived/keepalived.conf with following content on active load balancer\n# Following script will be used to check if haproxy is still running vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 # script check frequency weight 2 # step size for priority increase/decrease } # Create a virtual interface vrrp_instance vif1 { interface enp0s3 # the physical interface to attach to # get correct adapter name using ip addr/ifconfig command on your LB nodes. state MASTER # we will set this to BACKUP on the other machine priority 101 # we will set this to 100 on the other machine virtual_router_id 51 # needs to be the same on MASTER and BACKUP # The virtual ip address shared between the two loadbalancers virtual_ipaddress { 10.0.2.100 } # Use the script above to check if we should fail over track_script { chk_haproxy } } On the passive load-balancer create the file /etc/keepalived/keepalived.conf with following content:-\n# Following script will be used to check if haproxy is still running vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight 2 } # Create a virtual interface with same virtual_router_id as MASTER vrrp_instance vif1 { interface enp0s3 state BACKUP priority 100 virtual_router_id 51 # The virtual ip address shared between the two loadbalancers virtual_ipaddress { 10.0.2.100 } # Use the script above to check if we should fail over track_script { chk_haproxy } } Now if you check the enp0s3 interface on master LB node, you will see the floating IP is also assigned to the same interface. Testing Now from one of the webserver nodes try to curl http://10.0.2.100, you should get the web page. Next, stop the haproxy running on master LB, by either stopping the service sudo systemctl stop haproxy or even powering off the node, while trying to get the web page curl http://10.0.2.100 you will notice that the webpage becomes unavailable intermittently but then becomes available again. This is because the BACKUP haproxy gets promoted to MASTER. You can see the floating ip now shows up on the backup load-balancer node. You can even try watching the keepalived service logs on load-balancer-2 for when it gets promoted from BACKUP to MASTER using journalctl -fu keepalived.service\nNext steps Try setting up the two load balancers in active-active mode.\n","permalink":"https://abnvanand.github.io/posts/fail-over/","summary":"Introduction In this post we will setup loadbalancers in High Availability mode. We will be setting up the load balancers in active-passive fail-over configuration.\nI will be using haproxy as the load balancer and apache as the web server, but you can try with nginx as well.\nIn order to setup active-passive configuration we need some process which periodically checks the active load balancer and as soon as it goes down, it should promote the passive one to be active.","title":"Active-Passive Load Balancer"},{"content":"Introduction ENTRYPOINT is the command that is run CMD is the parameter passed to the command.\nFROM ubuntu ENTRYPOINT [\u0026#34;sleep\u0026#34;] CMD [\u0026#34;5m\u0026#34;] For example in this case we are overriding the default command of ubuntu docker image with sleep command and the default parameters for that command is 5m.\n","permalink":"https://abnvanand.github.io/posts/docker-cmd-vs-entrypoint/","summary":"Introduction ENTRYPOINT is the command that is run CMD is the parameter passed to the command.\nFROM ubuntu ENTRYPOINT [\u0026#34;sleep\u0026#34;] CMD [\u0026#34;5m\u0026#34;] For example in this case we are overriding the default command of ubuntu docker image with sleep command and the default parameters for that command is 5m.","title":"Dockerfile CMD vs ENTRYPOINT"}]